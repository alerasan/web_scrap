{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scraping methods\n",
    "1. Session initialization <br>\n",
    "2. Extracting links that lead to content, given some search query <br>\n",
    "3. Extracting content <br>\n",
    "4. Main function, that is used to extract all navigation links and all content. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 93.0.4577\n",
      "Get LATEST driver version for 93.0.4577\n",
      "There is no [win32] chromedriver for browser 93.0.4577 in cache\n",
      "Get LATEST driver version for 93.0.4577\n",
      "Trying to download new driver from https://chromedriver.storage.googleapis.com/93.0.4577.63/chromedriver_win32.zip\n",
      "Driver has been saved in cache [C:\\Users\\alloy\\.wdm\\drivers\\chromedriver\\win32\\93.0.4577.63]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import json\n",
    "import lxml.html\n",
    "\n",
    "links = []\n",
    "pictures = []\n",
    "videos = []\n",
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "#open the link to @Bee_nfluencer profile which I plan to discover \n",
    "username='bee_nfluencer'\n",
    "mainUrl = 'https://www.instagram.com/'+username+'/?hl=en'\n",
    "driver.get(mainUrl)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_email' from 'helpers' (C:\\opt\\conda\\envs\\mya\\lib\\site-packages\\helpers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-dd3f63343d88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0minstascrape\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mProfile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplatform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\opt\\conda\\envs\\mya\\lib\\site-packages\\instascrape\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0metree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mextract_email\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'extract_email' from 'helpers' (C:\\opt\\conda\\envs\\mya\\lib\\site-packages\\helpers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from instascrape import Post, Profile\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from sys import platform\n",
    "\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "def profile_push_button(browser):\n",
    "    try:\n",
    "        driver.find_element_by_css_selector(\"#react-root > section > main > div > div._2z6nI > div.Igw0E.rBNOH.eGOV_._4EzTm > div > button\").click()\n",
    "    except Exception:\n",
    "        print('no button found')\n",
    "    else:\n",
    "        print('button clicked')\n",
    "    time.sleep(5)\n",
    "    \n",
    "def profile_scroll_page(profile, browser, delay):\n",
    "    browser.get(profile.url)\n",
    "\n",
    "    source_data = []\n",
    "    js_script = (\n",
    "            \"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\"\n",
    "        )\n",
    "    profile_push_button(browser)\n",
    "    lenOfPage = browser.execute_script(js_script)\n",
    "    \n",
    "    match = False\n",
    "    while not match:\n",
    "        #time.sleep(abs(np.random.normal(3, 1.5)))\n",
    "        lastCount = lenOfPage\n",
    "        time.sleep(delay)\n",
    "        lenOfPage = browser.execute_script(js_script)\n",
    "        time.sleep(delay)\n",
    "        lenOfPage = browser.execute_script(js_script)\n",
    "        time.sleep(delay)\n",
    "        lenOfPage = browser.execute_script(js_script)\n",
    "        source_data.append(browser.page_source)\n",
    "        if lastCount == lenOfPage:\n",
    "            match = True\n",
    "            print(lastCount)\n",
    "    return source_data, browser\n",
    "\n",
    "def profile_seperate_posts(profile, source_data):\n",
    "\n",
    "    post_soup = []\n",
    "    for source in source_data[0]:\n",
    "        soup = BeautifulSoup(source, features=\"lxml\")\n",
    "            # posts = soup.find(\"span\", {\"id\":\"react-root\"})\n",
    "\n",
    "        anchor_tags = soup.find_all(\"a\")\n",
    "        found = [tag for tag in anchor_tags if tag.find(\"div\", {\"class\": \"eLAPa\"})]\n",
    "\n",
    "        for tag in found:\n",
    "            if tag not in post_soup:\n",
    "                post_soup.append(tag)\n",
    "    return post_soup\n",
    "\n",
    "def profile_create_post_objects(profile, post_soup, post_size):\n",
    "        # create post objects for each post on the page\n",
    "    profile_posts = []\n",
    "    for post in post_soup:\n",
    "        shortcode = post[\"href\"].replace(\"/p/\", \"\")[:-1]\n",
    "        profile_posts.append(Post(shortcode))\n",
    "        if len(profile_posts) >= post_size:\n",
    "            break\n",
    "            \n",
    "    return profile_posts\n",
    "\n",
    "def profile_grab_useful_data(profile_posts):\n",
    "    for i, post in enumerate(profile_posts):\n",
    "        if i % 10 == 0:\n",
    "            print(\"Read {} posts\".format(i))\n",
    "        try:\n",
    "            post.scrape()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "                \n",
    "def dynamic_load(profile, browser, max_posts=sys.maxsize, delay=3):\n",
    "        source_data = profile_scroll_page(profile, browser, delay)\n",
    "        post_soup = profile_seperate_posts(profile, source_data)\n",
    "        profile_posts = profile_create_post_objects(profile, post_soup, max_posts)\n",
    "        profile_grab_useful_data(profile_posts)\n",
    "        return profile_posts\n",
    "        #browser.close()\n",
    "\n",
    "max_posts_to_load = 500\n",
    "mydelay = 2\n",
    "\n",
    "my_prof = Profile(username)\n",
    "my_prof.scrape()\n",
    "print(my_prof.url)\n",
    "myprof_posts = dynamic_load(my_prof, driver, max_posts=max_posts_to_load, delay=mydelay)\n",
    "print(\"that's all\")\n",
    "print(myprof_posts)\n",
    "\n",
    "\n",
    "data_arr = []\n",
    "counter = 0\n",
    "for post in myprof_posts:\n",
    "    try:\n",
    "        counter = counter + 1\n",
    "        post.download(\"/Users/a/Desktop/py_web_scrap/posts_visual_content/\" + str(post.upload_date) + \".png\")\n",
    "        #I'll also create binary code of image in the post by following 3 lines\n",
    "        with open(\"/Users/a/Desktop/py_web_scrap/posts_visual_content/\" + str(post.upload_date) + \".png\",\"rb\") as image:\n",
    "            binaryf = Binary(image.read())\n",
    "            data_arr.append((post.url, post.upload_date, post.likes, post.comments, post.caption, post.tagged_users, post.hashtags, binaryf))    \n",
    "    except AttributeError as e: \n",
    "        pass \n",
    "    \n",
    "def to_csv_post_details(dataframe):\n",
    "    dataframe.to_csv('post_details.csv', header = True)\n",
    "\n",
    "\n",
    "dataframe = pd.DataFrame(data_arr, columns=['url', 'datetime', 'likes', 'comments', '1st comment', 'tagged', 'hashtags', 'binary_img'])\n",
    "dataframe = dataframe.sort_values(by=['datetime']).reset_index(drop=True)   #Sort by date\n",
    "dataframe  \n",
    "\n",
    "to_csv_post_details(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
